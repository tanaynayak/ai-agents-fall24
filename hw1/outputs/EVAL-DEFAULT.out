/home/tnayak2/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/tnayak2/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
[nltk_data] Downloading package punkt to /home/tnayak2/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
Evaluating with default generation parameters on 500 samples...
  0%|          | 0/63 [00:00<?, ?it/s]/home/tnayak2/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
  2%|▏         | 1/63 [00:01<01:34,  1.52s/it]  3%|▎         | 2/63 [00:02<01:23,  1.38s/it]  5%|▍         | 3/63 [00:04<01:20,  1.33s/it]  6%|▋         | 4/63 [00:05<01:17,  1.31s/it]  8%|▊         | 5/63 [00:06<01:15,  1.30s/it] 10%|▉         | 6/63 [00:07<01:13,  1.30s/it] 11%|█         | 7/63 [00:09<01:12,  1.29s/it] 13%|█▎        | 8/63 [00:10<01:10,  1.29s/it] 14%|█▍        | 9/63 [00:11<01:09,  1.29s/it] 16%|█▌        | 10/63 [00:13<01:07,  1.28s/it] 17%|█▋        | 11/63 [00:14<01:06,  1.28s/it] 19%|█▉        | 12/63 [00:15<01:05,  1.28s/it] 21%|██        | 13/63 [00:16<01:04,  1.28s/it] 22%|██▏       | 14/63 [00:18<01:02,  1.28s/it] 24%|██▍       | 15/63 [00:19<01:01,  1.29s/it] 25%|██▌       | 16/63 [00:20<01:00,  1.29s/it] 27%|██▋       | 17/63 [00:22<00:59,  1.29s/it] 29%|██▊       | 18/63 [00:23<00:57,  1.29s/it] 30%|███       | 19/63 [00:24<00:56,  1.29s/it] 32%|███▏      | 20/63 [00:25<00:55,  1.28s/it] 33%|███▎      | 21/63 [00:27<00:53,  1.28s/it] 35%|███▍      | 22/63 [00:28<00:52,  1.28s/it] 37%|███▋      | 23/63 [00:29<00:51,  1.28s/it] 38%|███▊      | 24/63 [00:31<00:49,  1.28s/it] 40%|███▉      | 25/63 [00:32<00:48,  1.28s/it] 41%|████▏     | 26/63 [00:33<00:47,  1.28s/it] 43%|████▎     | 27/63 [00:34<00:46,  1.28s/it] 44%|████▍     | 28/63 [00:36<00:44,  1.28s/it] 46%|████▌     | 29/63 [00:37<00:43,  1.28s/it] 48%|████▊     | 30/63 [00:38<00:42,  1.28s/it] 49%|████▉     | 31/63 [00:39<00:41,  1.28s/it] 51%|█████     | 32/63 [00:41<00:39,  1.28s/it] 52%|█████▏    | 33/63 [00:42<00:38,  1.28s/it] 54%|█████▍    | 34/63 [00:43<00:37,  1.28s/it] 56%|█████▌    | 35/63 [00:45<00:36,  1.29s/it] 57%|█████▋    | 36/63 [00:46<00:34,  1.29s/it] 59%|█████▊    | 37/63 [00:47<00:33,  1.29s/it] 60%|██████    | 38/63 [00:49<00:32,  1.29s/it] 62%|██████▏   | 39/63 [00:50<00:30,  1.29s/it] 63%|██████▎   | 40/63 [00:51<00:29,  1.29s/it] 65%|██████▌   | 41/63 [00:52<00:28,  1.29s/it] 67%|██████▋   | 42/63 [00:54<00:27,  1.29s/it] 68%|██████▊   | 43/63 [00:55<00:25,  1.28s/it] 70%|██████▉   | 44/63 [00:56<00:24,  1.28s/it] 71%|███████▏  | 45/63 [00:58<00:23,  1.29s/it] 73%|███████▎  | 46/63 [00:59<00:21,  1.29s/it] 75%|███████▍  | 47/63 [01:00<00:20,  1.29s/it] 76%|███████▌  | 48/63 [01:01<00:19,  1.28s/it] 78%|███████▊  | 49/63 [01:03<00:17,  1.28s/it] 79%|███████▉  | 50/63 [01:04<00:16,  1.28s/it] 81%|████████  | 51/63 [01:05<00:15,  1.29s/it] 83%|████████▎ | 52/63 [01:07<00:14,  1.28s/it] 84%|████████▍ | 53/63 [01:08<00:12,  1.29s/it] 86%|████████▌ | 54/63 [01:09<00:11,  1.29s/it] 87%|████████▋ | 55/63 [01:10<00:10,  1.29s/it] 89%|████████▉ | 56/63 [01:12<00:09,  1.29s/it] 90%|█████████ | 57/63 [01:13<00:07,  1.29s/it] 92%|█████████▏| 58/63 [01:14<00:06,  1.29s/it] 94%|█████████▎| 59/63 [01:16<00:05,  1.29s/it] 95%|█████████▌| 60/63 [01:17<00:03,  1.28s/it] 97%|█████████▋| 61/63 [01:18<00:02,  1.28s/it] 98%|█████████▊| 62/63 [01:19<00:01,  1.28s/it]100%|██████████| 63/63 [01:21<00:00,  1.28s/it]100%|██████████| 63/63 [01:21<00:00,  1.29s/it]
/home/tnayak2/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
      rouge1    rouge2     rougeL  rougeLsum      bleu  bertscore_precision  bertscore_recall  bertscore_f1
0  13.879434  1.789572  11.627012  11.198663  0.857749            82.141097         81.434677      81.76077
